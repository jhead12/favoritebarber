version: '3.8'
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: rateyourbarber
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - 5432:5432

  redis:
    image: redis:7
    ports:
      - 6379:6379

  api:
    build:
      context: .
      dockerfile: api/Dockerfile
    command: npm run dev
    environment:
      DATABASE_URL: "postgresql://postgres:password@postgres:5432/rateyourbarber"
      REDIS_URL: "redis://redis:6379"
      LOG_LEVEL: ${LOG_LEVEL:-info}
      SENTRY_DSN: ${SENTRY_DSN}
      USE_YELP_GRAPHQL: ${USE_YELP_GRAPHQL}
      ENABLE_COST_TRACKING: ${ENABLE_COST_TRACKING}
    env_file:
      - ./.env
    ports:
      - 3000:3000
    depends_on:
      - postgres
      - redis

  web:
    build: ./web
    command: npm run dev
    environment:
      # For client-side requests from the browser, use the host-mapped API address.
      # Set to http://localhost:3000 so the browser can resolve it. If you run the API separately,
      # update this to the host:port where the API is reachable from your browser.
      # When running in compose, the web container should reach the API by service name for server-side code,
      # while the browser needs a host-accessible URL. Set both variables: `API_INTERNAL_URL` for server
      # rewrites and `NEXT_PUBLIC_API_URL` for client-side requests.
      - API_INTERNAL_URL=http://api:3000
      - NEXT_PUBLIC_API_URL=http://localhost:3000
    ports:
      - 3001:3000
    depends_on:
      - api

  worker:
    build: ./workers
    command: npm run dev
    environment:
      DATABASE_URL: "postgresql://postgres:password@postgres:5432/rateyourbarber"
      REDIS_URL: "redis://redis:6379"
      YELP_API_KEY: ${YELP_API_KEY}
      OLLAMA_ENDPOINT: ${OLLAMA_ENDPOINT:-http://host.docker.internal:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.2:3b}
    env_file:
      - ./.env
    depends_on:
      - postgres
      - redis

  # Local Ollama for Llama LLM (optional)
  # To enable, uncomment this block and run: ollama serve
  # Or use: podman-compose up ollama
  #ollama:
  #  image: ollama/ollama:latest
  #  ports:
  #    - "11434:11434"
  #  volumes:
  #    - ollama-data:/root/.ollama
  #  environment:
  #    - OLLAMA_MODELS=/root/.ollama/models

# ML worker (optional)
# To enable the ML worker, remove the leading '#' characters on this block
# and ensure `ml-worker/` exists with a Dockerfile or change `image:` to your registry image.
#ml-worker:
#  build: ./ml-worker
#  command: python app.py
#  volumes:
#    - ./ml-worker:/usr/src/app
#  environment:
#    - MODEL_PATH=/models
#    - REDIS_URL=${REDIS_URL}
#    - DATABASE_URL=${DATABASE_URL}
#  ports:
#    - 5000:5000
#  depends_on:
#    - redis
#    - postgres

volumes:
  pgdata:
  ollama-data:

  # Optional monitoring stack (commented by default). Uncomment to run local Prometheus + Grafana.
# prometheus:
#   image: prom/prometheus:latest
#   ports:
#     - 9090:9090
#   volumes:
#     - ./ops/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
#   depends_on:
#     - api
#
# grafana:
#   image: grafana/grafana:latest
#   ports:
#     - 3002:3000
#   environment:
#     - GF_SECURITY_ADMIN_PASSWORD=admin
#   depends_on:
#     - prometheus
